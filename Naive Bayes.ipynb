{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Sentiments analysis using naive bayes. \n",
    "\n",
    "Download link: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv\n",
    "\n",
    "About Dataset\n",
    "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.\n",
    "For more dataset information, please go through the following link,\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a691375282ef6d63"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-28T17:39:05.522753Z",
     "start_time": "2024-06-28T17:39:03.835397Z"
    }
   },
   "id": "initial_id",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../Downloads/Data for ML/IMDB Dataset.csv\")\n",
    "print(df.head(3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T14:50:04.888339Z",
     "start_time": "2024-06-28T14:50:04.379726Z"
    }
   },
   "id": "dffa0e50055d40b3",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Info of the data. \n",
    "\n",
    "--> To check about the missing values. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24c34d42a0ed8fc0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()                       # There are no missing values present in the data. "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T15:00:10.388080Z",
     "start_time": "2024-06-28T15:00:10.319775Z"
    }
   },
   "id": "135b74141b61f193",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text preprocessing. \n",
    "\n",
    "--> Removal html tags.\n",
    "--> Removal special characters. \n",
    "--> Converting everything to the lower  case. \n",
    "--> Removal of stop words. \n",
    "--> Stemming. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5361d93b05f79422"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Removal of html tags. \n",
    "def rem_html(text: str) -> str:\n",
    "    eg_text = re.compile(\"<.*?>\")   \n",
    "    return re.sub(eg_text, \" \", text)\n",
    "\n",
    "def rem_specialChar(text: str) -> str:\n",
    "    #Specific\n",
    "    phrase = re.sub(r\"won't\", r\"will not\", text)\n",
    "    phrase = re.sub(r\"don't\", r\"do not\", phrase)\n",
    "    phrase = re.sub(r\"can't\", r\"can not\", phrase)\n",
    "    \n",
    "    #General\n",
    "    phrase = re.sub(r\"n't\", r\" not\", phrase)\n",
    "    phrase = re.sub(r\"'s\", r\" is\", phrase)\n",
    "    phrase = re.sub(r\"'m\", r\" am\", phrase)\n",
    "    phrase = re.sub(r\"'re\", r\" are\", phrase)\n",
    "    phrase = re.sub(r\"'ll\", r\" will\", phrase)\n",
    "    phrase = re.sub(r\"'t\", r\" not\", phrase)\n",
    "    phrase = re.sub(r\"'ve\", r\" have\", phrase)\n",
    "    \n",
    "    # Clean punctuations\n",
    "    phrase = re.sub(r'[?|!|\\'|\"|#|@|:]', r'', phrase)\n",
    "    phrase = re.sub(r'[(|)|.|,|\\|/]', r'', phrase)\n",
    "    phrase = re.sub(r'-', r' ', phrase)    \n",
    "    \n",
    "    # Special characters (Remove all words which are not in that range)\n",
    "    phrase = re.sub(r'[^A-Za-z0-9]+', r' ', phrase)\n",
    "    \n",
    "    # Remove all the alphanumeric words\n",
    "    phrase = re.sub(r'\\S*\\d\\S*', r'', phrase)\n",
    "    return phrase\n",
    "\n",
    "def lowercase(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def tokens(text: str) -> list[str]:\n",
    "    return text.split()\n",
    "\n",
    "def rem_stopwords(text: list[str]) -> list[str]:\n",
    "    stop = stopwords.words('english')\n",
    "    return [word for word in text if word not in stop]\n",
    "\n",
    "def rem_stemmer(text: list[str]) -> list[str]:\n",
    "    stemmer = PorterStemmer()\n",
    "    ls: list = []\n",
    "    for word in text:\n",
    "        ls.append(stemmer.stem(word))\n",
    "    return ls\n",
    "\n",
    "def join_words(text: list[str]) -> str:\n",
    "    return \" \".join(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T18:28:58.646520Z",
     "start_time": "2024-06-28T18:28:58.628471Z"
    }
   },
   "id": "5ec0cbef3aef16b6",
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here I am applying text preprocessing in all the data. This is not the correct way of preprocessing the data (data leakage problem) but for text data it is fine. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce7c40cce203ff4c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  [one, review, mention, watch, oz, episod, hook...  positive\n",
      "1  [wonder, littl, product, film, techniqu, unass...  positive\n",
      "2  [thought, wonder, way, spend, time, hot, summe...  positive\n"
     ]
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(rem_html)\n",
    "df['review'] = df['review'].apply(rem_specialChar)\n",
    "df['review'] = df['review'].apply(lowercase)\n",
    "df['review'] = df['review'].apply(tokens)\n",
    "df['review'] = df['review'].apply(rem_stopwords)\n",
    "df['review'] = df['review'].apply(rem_stemmer)\n",
    "print(df.head(3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T18:16:39.373090Z",
     "start_time": "2024-06-28T18:15:12.231608Z"
    }
   },
   "id": "1e9a4f124518a500",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(join_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T18:30:49.400969Z",
     "start_time": "2024-06-28T18:30:48.632803Z"
    }
   },
   "id": "329239ba26794ca6",
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now convert the sentiments into 0 (negative) and 1 (positive)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18f80e551c028bf3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  one review mention watch oz episod hook right ...          1\n",
      "1  wonder littl product film techniqu unassum old...          1\n",
      "2  thought wonder way spend time hot summer weeke...          1\n"
     ]
    }
   ],
   "source": [
    "df['sentiment'].replace({'positive': 1, 'negative': 0}, inplace=True)\n",
    "print(df.head(3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T18:36:24.646671Z",
     "start_time": "2024-06-28T18:36:24.640040Z"
    }
   },
   "id": "d762d1a6feb599d1",
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it's time to convert each unique word into unique feature. Here we are making our own vocabulary or text corpus or dictionary based on the data we have.    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ae77c12eb9876c0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 97590)\n",
      "Number of data points: 50000\n",
      "Number of unique words/features: 97590\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df['review']).toarray()\n",
    "print(X.shape)\n",
    "print(f\"Number of data points: {X.shape[0]}\")\n",
    "print(f\"Number of unique words/features: {X.shape[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T19:07:53.102198Z",
     "start_time": "2024-06-28T19:07:46.784543Z"
    }
   },
   "id": "b2f5c36bb943c697",
   "execution_count": 81
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
